from matplotlib import pyplot as plt
from sklearn.metrics import confusion_matrix
from logistic.esitmation import Estimation
from matplotlib.colors import ListedColormap
import numpy as np
# from sklearn.learning_curve import learning_curve
# from sklearn.learning_curve import validation_curve
# plot confusion matrix and return the dictionary of confusion matrix
def plot_confusion_matrix(label,predict,negative_label,positive_label,param):
    cf_matrix = confusion_matrix(label, predict)
    print('confusion matrix:')
    print(cf_matrix)
    print('\n')
    plt.matshow(cf_matrix)
    plt.title('Confusion matrix with C=%.2f'%param)
    plt.colorbar()
    plt.ylabel(u'real type(0 for %s, 1 for %s)'%(negative_label,positive_label))
    plt.xlabel(u'predictive type(0 for %s, 1 for %s)'%(negative_label,positive_label))
    plt.grid(False)
    plt.show()


    TP = cf_matrix[1, 1]
    TN = cf_matrix[0, 0]
    FP = cf_matrix[0, 1]
    FN = cf_matrix[1, 0]

    return {'TN':TN, 'TP':TP, 'FP':FP, 'FN':FN}

# return a dict of all kinds of scores for estimation and the desciption of these scores
def estimate_scores(label,predict):
    myEstimation = Estimation(label, predict)

    scores = myEstimation.all_score()

    myEstimation.plot_roc(label,predict)

    reurn_scores = scores.copy()

    return reurn_scores



def plot_decision_regions(X, y, classifier,labels,legend,resolution=0.02):
    """plot the decision regions.

          Parameters
          ----------
          X : {array-like, sparse matrix}, shape (n_samples, n_features)
              Training vector, where n_samples in the number of samples and
              n_features is the number of features.

          y : array-like, shape (n_samples,)
              Label vector relative to X.

          classifier: The instance of classifier, which has been trained.
                    E.g. LogisticRegression, Perceptron

          labels: The labels for xlabel and ylabel

          legend: The legend string for each ploted line

          resolution: just for plotting the decision line

          """

    # setup marker generator and color map
    markers = ('s','x','o','^','v')
    colors = ('red', 'blue','lightgreen','gray','cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])   # choose the colormap according to the number of classes

    # plot the decision surface
    x1_min, x1_max = X[:,0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:,1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min,x1_max,resolution),
                           np.arange(x2_min,x2_max,resolution))
    Z = classifier.predict(np.array([xx1.ravel(),xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())

    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        plt.scatter(x=X[y == cl,0], y=X[y == cl,1],
                    alpha=0.8, c=cmap(idx),
                    marker=markers[idx],label=legend[cl])
    # set labels
    plt.xlabel(labels[0])
    plt.ylabel(labels[1])

def plot_validation_curve(param_range, train_scores, test_scores,param_name,score_name,positive_label):
    '''
        Help choose the hyperparameters of the model by validation curve,
        the figure is about (#value_parameters,scores)
        If the scores are generated by kfold, then scores.shape[1] = cv

        :param param_range: the value of parameter for each iterations with shape (1, #param_range)
        :param train_scores:  the score of each iterations with shape (#param_range, #iterations)
        :param test_scores:  the score of each iterations with shape (#param_range, #iterations)
        :param param_name: the name of parameter which are estimated
        :param score_name: the estimation baseline
        :param positive_label: the integer which indicates the positive label
        :return:
    '''
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    test_mean = np.mean(test_scores, axis=1)
    test_std = np.std(test_scores, axis=1)

    plt.figure()
    plt.plot(param_range,train_mean,color='blue',marker='o',markersize=5,label='training accuracy')
    plt.fill_between(param_range, train_mean+ train_std, train_mean-train_std,alpha=0.15,color='blue')
    plt.plot(param_range,test_mean,color='green',linestyle='--',marker='s',markersize=5, label='validation accuracy')
    plt.fill_between(param_range,test_mean+test_std, test_mean-test_std, alpha=0.15, color='green')

    plt.xlabel('Parameter '+param_name)
    plt.ylabel('Accuracy(score=%s, postive label=%s)' % (score_name, positive_label))
    plt.xscale('log')   # rescale the x-axis in logarithm

    plt.legend(loc='lower right')
    plt.ylim([0.8,1.0])
    plt.show()

def plot_leaning_curve(train_sizes, train_scores, test_scores,score_name,positive_label):
    '''
    Determine whether the model is under-fitting or over-fitting by learning curve,
    the figure about the (#case_size,scores)
    If the scores are generated by kfold, then #iterations = cv

    :param train_sizes: the #case_size for each iterations with shape (1,#iterations)
    :param train_scores:  the score of each iterations with shape ( #train_sizes,#iterations)
    :param test_scores:  the score of each iterations with shape ( #train_sizes,#iterations)
    :return: 
    '''
    train_mean = np.mean(train_scores, axis=1)
    train_std = np.std(train_scores, axis=1)
    test_mean = np.mean(test_scores, axis=1)
    test_std = np.std(test_scores, axis=1)

    plt.figure()
    plt.plot(train_sizes,train_mean,color='blue',marker='o',markersize=5,label='training accuracy')
    plt.fill_between(train_sizes, train_mean+ train_std, train_mean-train_std,alpha=0.15,color='blue')
    plt.plot(train_sizes,test_mean,color='green',linestyle='--',marker='s',markersize=5, label='validation accuracy')
    plt.fill_between(train_sizes,test_mean+test_std, test_mean-test_std, alpha=0.15, color='green')

    plt.xlabel('Number of training samples')
    plt.ylabel('Accuracy(score=%s, postive label=%s)' % (score_name,positive_label))

    # plt.title('The learning curv')

    plt.legend(loc='lower right')
    plt.ylim([0.8,1.0])
    plt.show()


def plot_importances_histogram(feat_labels, importances):
    feat_size = importances.shape[0]
    indices =  np.argsort(importances)[::-1]    # invert to make the biggest one at the first place and reshape

    for f in range(feat_size):
        print("%2d) %-*s %f"%(f+1, 30, feat_labels[f],importances[indices[f]]))

    plt.figure()
    plt.title('Feature Importances')
    plt.bar(range(feat_size), importances[indices], color='lightblue', align='center')
    plt.xticks(range(feat_size), feat_labels, rotation=90)
    plt.xlim([-1, feat_size])
    plt.tight_layout()
    plt.show()


